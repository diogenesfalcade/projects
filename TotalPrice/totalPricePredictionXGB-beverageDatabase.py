# -*- coding: utf-8 -*-
"""BigDataSpark-ARA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WoiQ-D4qbUcBFv1Z7FUvXM85JMPFO59Z
"""

from pyspark.sql import SparkSession
from pyspark.ml.regression import RandomForestRegressor, GBTRegressor, LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.sql.functions import col, expr, approx_percentile, sum as spark_sum
import seaborn as sns
import matplotlib.pyplot as plt
import xgboost as xgb
import pandas as pd

spark = SparkSession.builder \
    .appName("SyntheticBeverageSales") \
    .getOrCreate()

# Montar Google Drive
# from google.colab import drive
# drive.mount('/content/drive')

# https://www.kaggle.com/datasets/sebastianwillmann/beverage-sales
file_path = '/content/drive/My Drive/Dados_ML_dsbd/synthetic_beverage_sales_data.csv'

try:
  data = spark.read.csv(file_path, header=True, inferSchema=True)
  data.printSchema()
  data.show(5)
except Exception as e:
  print(f"An error occurred: {e}")

print('Number of rows: \t', data.count())
print('Number of columns: \t', len(data.columns))

"""### Remoção de outliers utilizando a técnica do intervalo interquartil (IQR)"""

def remove_outliers_spark(df, numeric_columns, accuracy=100):
    bounds = {}

    # Calcula Q1 e Q3 para cada coluna numérica
    for col_name in numeric_columns:
        percentiles = df.select(approx_percentile(col_name, [0.25, 0.75], accuracy)).first()[0]
        q1, q3 = percentiles  # desempacotando os percentis
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        bounds[col_name] = (lower_bound, upper_bound)

    # Aplica os limites ao DataFrame
    for col_name, (lower, upper) in bounds.items():
        df = df.filter((col(col_name) >= lower) & (col(col_name) <= upper))

    # Visualizar boxplots das colunas numéricas sem outliers
    num_columns = len(numeric_columns)
    fig, axes = plt.subplots(1, num_columns, figsize=(18, 6))

    for i, col_name in enumerate(numeric_columns):
        # Coleta dados para visualização local
        col_data = df.select(col_name).rdd.flatMap(lambda x: x).collect()
        sns.boxplot(x=col_data, ax=axes[i])
        axes[i].set_title(f'Boxplot for {col_name}')

    plt.tight_layout()
    plt.show()

    return df

# Limpar outliers
data = remove_outliers_spark(data, ["Unit_Price", "Quantity", "Discount", "Total_Price"])

# Salvar no Google Drive
data.write.parquet("/content/drive/MyDrive/cleaned_data.parquet", mode="overwrite")

# Carregar do Google Drive
data = spark.read.parquet("/content/drive/MyDrive/cleaned_data.parquet")

data.groupBy("Category").count().show()

def plot_category_sales_spark(df, category_column, sales_column):
    # Agrupar por categoria e somar as vendas
    category_sales = (
        df.groupBy(category_column)
        .agg(spark_sum(col(sales_column)).alias("Total_Sales"))
        .orderBy(col("Total_Sales").desc())
    )

    # Coletar os dados para o driver
    category_sales_pd = category_sales.toPandas()

    # Plotar o gráfico
    plt.figure(figsize=(10, 6))
    colors = plt.cm.Paired(range(len(category_sales_pd)))
    plt.bar(category_sales_pd[category_column], category_sales_pd["Total_Sales"], color=colors)
    plt.title('Total Sales by Category')
    plt.ylabel('Total Sales')
    plt.xlabel('Category')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

plot_category_sales_spark(data, "Category", "Total_Price")

def plot_price_vs_quantity_spark(df, x_column, y_column):
    # Converter o DataFrame do Spark para Pandas
    df_pd = df.select(x_column, y_column).toPandas()

    # Criar o gráfico de dispersão
    plt.figure(figsize=(8, 6))
    plt.scatter(df_pd[x_column], df_pd[y_column], color="#add8e6", edgecolors = "black", alpha=0.7)
    plt.title('Price vs Quantity')
    plt.xlabel('Unit Price')
    plt.ylabel('Quantity')
    plt.tight_layout()
    plt.show()

"""### A primeira vista o gráfico parece representar que existe um valor máximo de compra. Porém, a curva exibida nada mais representa do que o IQR aplicado."""

plot_price_vs_quantity_spark(data, "Unit_Price", "Quantity")

"""## Seleção de features

##### Teoricamente, regressão linear deveria ser suficiente, visto que o preço total nada mais é que a combinação de quantidade e preço. Porém, temos diversas regiões, compradores e produtos na Alemanha, o que implica em um preço diferente para cada produto em cada região, tornando então as outras colunas relevantes para a predição do preço total.
"""

# Testando Regressão linear com apenas as features numéricas
# Selecionar os 5 mil registros
fractions = 5000 / data.count()
sample_data = data.sample(withReplacement=False, fraction=fractions, seed=42)
sample_data = sample_data.limit(5000)
columns_to_include = ["Unit_Price", "Quantity", "Discount", "Total_Price"]
data_selected = sample_data.select(*columns_to_include)
data_clean = data_selected.dropna()

# Combinar as features em um vetor único
feature_columns = ["Unit_Price", "Quantity", "Discount"]
assembler = VectorAssembler(inputCols=feature_columns, outputCol="features")
data_with_features = assembler.transform(data_clean)

# Preparar os dados para o modelo
final_data = data_with_features.select("features", "Total_Price")
train_data, test_data = final_data.randomSplit([0.8, 0.2], seed=42)

# Criar e treinar o modelo de regressão linear
lr = LinearRegression(
                      featuresCol="features",
                      labelCol="Total_Price"
                      )
lr_model = lr.fit(train_data)

# Avaliar o modelo no conjunto de teste
test_results = lr_model.evaluate(test_data)
print(f"R²: {test_results.r2}")
print(f"RMSE: {test_results.rootMeanSquaredError}")
print(f"Intercepto: {lr_model.intercept}")
print(f"Coeficientes: {lr_model.coefficients}")

# Selecionando features relevantes e criando o VectorAssembler
selected_columns = ["Customer_Type", "Product", "Category", "Unit_Price", "Quantity", "Discount", "Region", "Total_Price"]
data = data.select(*selected_columns)

# Indexar colunas categóricas
indexers = [
    StringIndexer(inputCol="Customer_Type", outputCol="Customer_Type_Index"),
    StringIndexer(inputCol="Product", outputCol="Product_Index"),
    StringIndexer(inputCol="Category", outputCol="Category_Index"),
    StringIndexer(inputCol="Region", outputCol="Region_Index"),
]

for indexer in indexers:
    data = indexer.fit(data).transform(data)

# Criar vetor de features
feature_columns = [
    "Customer_Type_Index", "Product_Index", "Category_Index",
    "Unit_Price", "Quantity", "Discount", "Region_Index"
]
assembler = VectorAssembler(inputCols=feature_columns, outputCol="features")
data = assembler.transform(data)

# Divisão em amostra e conjunto completo
fractions = 5000 / data.count()
sample_data = data.sample(withReplacement=False, fraction=fractions, seed=42)
sample_data = sample_data.limit(5000)
full_data = data

# Dividir os conjuntos em treino e teste
train_sample, test_sample = sample_data.randomSplit([0.8, 0.2], seed=42)
train_full, test_full = full_data.randomSplit([0.8, 0.2], seed=42)

# Criar e treinar o modelo de regressão linear
lr = LinearRegression(
                      featuresCol="features",
                      labelCol="Total_Price"
                      )
lr_model = lr.fit(train_sample)

# Avaliar o modelo no conjunto de teste
test_results = lr_model.evaluate(test_sample)
print(f"R²: {test_results.r2}")
print(f"RMSE: {test_results.rootMeanSquaredError}")
print(f"Intercepto: {lr_model.intercept}")
print(f"Coeficientes: {lr_model.coefficients}")

# Criar e treinar o modelo de regressão linear
lr = LinearRegression(
                      featuresCol="features",
                      labelCol="Total_Price"
                      )
lr_model = lr.fit(train_full)

# Avaliar o modelo no conjunto de teste
test_results = lr_model.evaluate(test_full)
print(f"R²: {test_results.r2}")
print(f"RMSE: {test_results.rootMeanSquaredError}")
print(f"Intercepto: {lr_model.intercept}")
print(f"Coeficientes: {lr_model.coefficients}")

def fit_model(model, train_data, test_data):
    model = model.fit(train_data)
    predictions = model.transform(test_data)
    evaluator = RegressionEvaluator(labelCol="Total_Price", predictionCol="prediction", metricName="rmse")
    rmse = evaluator.evaluate(predictions)
    evaluator_r2 = RegressionEvaluator(labelCol="Total_Price", predictionCol="prediction", metricName="r2")
    r2 = evaluator_r2.evaluate(predictions)
    print(f"RMSE: {rmse}")
    print(f"R²: {r2}")
    return model, predictions, rmse, r2

# Random forest com 5000 registros
rf = RandomForestRegressor(featuresCol="features", labelCol="Total_Price", predictionCol="prediction", maxBins = 64, seed = 1234)
rf_sample, rf_sample_predictions, rmse_rf_sample, r2_rf_sample = fit_model(rf, train_sample, test_sample)

# Random forest com todos os registros
rf_full, rf_full_predictions, rmse_rf_full, r2_rf = fit_model(rf, train_full, test_full)

# Gradient Boosting com 5000 registros
gbt = GBTRegressor(featuresCol="features", labelCol="Total_Price", maxIter=20, subsamplingRate = 0.8, maxBins = 64, seed = 111) #lossType squared/absolute, maxDepth, stepSize
gbt_sample, gbt_sample_predictions, rmse_gbt_sample, r2_gbt_sample = fit_model(gbt, train_sample, test_sample)

# Gadrient Boosting com todos os registros
gbt_full, gbt_full_predictions, rmse_gbt_full, r2_gbt_full = fit_model(gbt, train_full, test_full)

predictions_and_labels = gbt_full_predictions.select("prediction", "Total_Price").toPandas()

plt.figure(figsize=(8, 8))
plt.scatter(predictions_and_labels["Total_Price"], predictions_and_labels["prediction"], alpha=0.6)
plt.plot([predictions_and_labels["Total_Price"].min(), predictions_and_labels["Total_Price"].max()],
         [predictions_and_labels["Total_Price"].min(), predictions_and_labels["Total_Price"].max()],
         color="red", linestyle="--", linewidth=2)
plt.xlabel("Valores Reais")
plt.ylabel("Valores Preditos")
plt.title("Valores Reais vs. Valores Preditos")
plt.show()

predictions_and_labels = gbt_sample_predictions.select("prediction", "Total_Price").toPandas()

plt.figure(figsize=(8, 8))
plt.scatter(predictions_and_labels["Total_Price"], predictions_and_labels["prediction"], alpha=0.6)
plt.plot([predictions_and_labels["Total_Price"].min(), predictions_and_labels["Total_Price"].max()],
         [predictions_and_labels["Total_Price"].min(), predictions_and_labels["Total_Price"].max()],
         color="red", linestyle="--", linewidth=2)
plt.xlabel("Valores Reais")
plt.ylabel("Valores Preditos")
plt.title("Valores Reais vs. Valores Preditos")
plt.show()

"""# Colapse"""

metrics_data = {
    "Modelo": ["Gradient Boosting - Amostra", "Gradient Boosting - Conjunto Completo"],
    "RMSE": [rmse_gbt_sample, rmse_gbt_full],
    "R²": [r2_gbt_sample, r2_gbt_full]
}

metrics_df = pd.DataFrame(metrics_data)
print(metrics_df)

spark.stop()